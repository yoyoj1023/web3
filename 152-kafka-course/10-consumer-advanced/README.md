# 10 - Consumer é€²éšé…ç½®

## å­¸ç¿’ç›®æ¨™

- è‡ªå‹•æäº¤ vs æ‰‹å‹•æäº¤çš„æ¬Šè¡¡
- Offset é‡ç½®ç­–ç•¥
- æ¶ˆè²»è€…ä¸¦ç™¼è™•ç†
- å¯¦ç¾å¯é çš„ Consumer

## è‡ªå‹•æäº¤ vs æ‰‹å‹•æäº¤

### å°æ¯”

| ç‰¹æ€§ | è‡ªå‹•æäº¤ | æ‰‹å‹•æäº¤ |
|------|---------|----------|
| **å¯¦ç¾è¤‡é›œåº¦** | ç°¡å–® | è¼ƒè¤‡é›œ |
| **æ€§èƒ½** | è¼ƒé«˜ | ç¨ä½ |
| **å¯é æ€§** | å¯èƒ½ä¸Ÿå¤±æˆ–é‡è¤‡ | æ›´å¯æ§ |
| **é©ç”¨å ´æ™¯** | å…è¨±å°‘é‡ä¸Ÿå¤± | ä¸èƒ½ä¸Ÿå¤±æ•¸æ“š |

### è‡ªå‹•æäº¤ç¤ºä¾‹

```typescript
import { Kafka } from 'kafkajs';

const consumer = kafka.consumer({
  groupId: 'auto-commit-group',
  // è‡ªå‹•æäº¤é…ç½®ï¼ˆé»˜èªï¼‰
  autoCommit: true,
  autoCommitInterval: 5000,  // æ¯ 5 ç§’æäº¤ä¸€æ¬¡
  autoCommitThreshold: 100   // æˆ–æ¯ 100 æ¢æ¶ˆæ¯æäº¤
});

await consumer.run({
  eachMessage: async ({ message }) => {
    // è™•ç†æ¶ˆæ¯
    await processMessage(message);
    // Offset æœƒè‡ªå‹•æäº¤ï¼ˆä¸éœ€è¦æ‰‹å‹•æ“ä½œï¼‰
  }
});
```

**é¢¨éšª**ï¼š
```
æ™‚é–“ç·šï¼š
0s:  è™•ç† offset 0-99
5s:  è‡ªå‹•æäº¤ offset 100
6s:  è™•ç† offset 100-150 æ™‚ç¨‹åºå´©æ½°
é‡å•Ÿ: å¾ offset 100 é–‹å§‹ï¼ˆoffset 100-150 ä¸Ÿå¤±ï¼‰
```

### æ‰‹å‹•æäº¤ç¤ºä¾‹

å‰µå»º `examples/consumer-advanced/01-manual-commit.ts`ï¼š

```typescript
import { Kafka } from 'kafkajs';

const kafka = new Kafka({
  clientId: 'manual-commit-demo',
  brokers: ['localhost:9092']
});

const consumer = kafka.consumer({
  groupId: 'manual-commit-group',
  autoCommit: false  // é—œé–‰è‡ªå‹•æäº¤
});

async function run() {
  await consumer.connect();
  await consumer.subscribe({ topic: 'orders', fromBeginning: false });

  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      try {
        const order = JSON.parse(message.value?.toString() || '{}');
        
        // 1. è™•ç†æ¶ˆæ¯
        await processOrder(order);
        
        // 2. å¯«å…¥æ•¸æ“šåº«
        await saveToDatabase(order);
        
        // 3. æ‰‹å‹•æäº¤ offsetï¼ˆç¢ºä¿è™•ç†æˆåŠŸå¾Œæ‰æäº¤ï¼‰
        await consumer.commitOffsets([{
          topic,
          partition,
          offset: (parseInt(message.offset) + 1).toString()
        }]);
        
        console.log(`âœ… è™•ç†ä¸¦æäº¤: offset ${message.offset}`);

      } catch (error) {
        console.error(`âŒ è™•ç†å¤±æ•—: offset ${message.offset}`, error);
        // ä¸æäº¤ offsetï¼Œä¸‹æ¬¡æœƒé‡æ–°è™•ç†
        // ä½†è¦æ³¨æ„é¿å…ç„¡é™é‡è©¦
      }
    }
  });
}

async function processOrder(order: any) {
  // æ¥­å‹™é‚è¼¯
  console.log(`è™•ç†è¨‚å–®: ${order.orderId}`);
  await new Promise(resolve => setTimeout(resolve, 100));
}

async function saveToDatabase(order: any) {
  // æŒä¹…åŒ–
  console.log(`ä¿å­˜è¨‚å–®: ${order.orderId}`);
  await new Promise(resolve => setTimeout(resolve, 50));
}

run();
```

### æ‰¹æ¬¡æ‰‹å‹•æäº¤

```typescript
await consumer.run({
  eachBatch: async ({ batch, resolveOffset, commitOffsetsIfNecessary }) => {
    for (const message of batch.messages) {
      await processMessage(message);
      resolveOffset(message.offset);
    }
    
    // æ‰¹æ¬¡è™•ç†å®Œæˆå¾Œä¸€æ¬¡æ€§æäº¤
    await commitOffsetsIfNecessary();
  }
});
```

## Offset é‡ç½®ç­–ç•¥

### earliest vs latest

```typescript
const consumer = kafka.consumer({
  groupId: 'my-group',
  autoOffsetReset: 'earliest'  // æˆ– 'latest'
});
```

| ç­–ç•¥ | è¡Œç‚º | é©ç”¨å ´æ™¯ |
|------|------|---------|
| **earliest** | å¾æœ€æ—©çš„æ¶ˆæ¯é–‹å§‹ | ä¸èƒ½æ¼æ¶ˆæ¯ |
| **latest** | å¾æœ€æ–°çš„æ¶ˆæ¯é–‹å§‹ | åªé—œå¿ƒæ–°æ¶ˆæ¯ |

### å ´æ™¯åˆ†æ

```
Partition ç‹€æ…‹ï¼š
Offset: 0 ....... 1000 ....... 2000 (æœ€æ–°)

å ´æ™¯ 1: æ–° Consumer Groupï¼ˆæ²’æœ‰committed offsetï¼‰
  earliest â†’ å¾ offset 0 é–‹å§‹
  latest   â†’ å¾ offset 2000 é–‹å§‹

å ´æ™¯ 2: å·²æœ‰ committed offset = 1500
  ç„¡è«–é…ç½®å¦‚ä½• â†’ å¾ offset 1500 é–‹å§‹

å ´æ™¯ 3: committed offset = 500ï¼Œä½†æœ€æ—©å¯ç”¨ offset = 1000
  earliest â†’ å¾ offset 1000 é–‹å§‹ï¼ˆæ•¸æ“šå·²è¢«æ¸…ç†ï¼‰
```

## ä¸¦ç™¼è™•ç†

### æ–¹å¼ 1ï¼šå¤š Consumer å¯¦ä¾‹

```bash
# å•Ÿå‹•å¤šå€‹é€²ç¨‹ï¼Œæ¯å€‹é‹è¡Œä¸€å€‹ Consumer
node consumer.js  # Consumer 1
node consumer.js  # Consumer 2
node consumer.js  # Consumer 3
```

### æ–¹å¼ 2ï¼šeachBatch + ä¸¦ç™¼è™•ç†

å‰µå»º `examples/consumer-advanced/02-concurrent-processing.ts`ï¼š

```typescript
import { Kafka } from 'kafkajs';
import pLimit from 'p-limit';

const kafka = new Kafka({
  clientId: 'concurrent-demo',
  brokers: ['localhost:9092']
});

const consumer = kafka.consumer({ groupId: 'concurrent-group' });

async function run() {
  await consumer.connect();
  await consumer.subscribe({ topic: 'orders', fromBeginning: false });

  await consumer.run({
    eachBatch: async ({ batch, resolveOffset, heartbeat }) => {
      // é™åˆ¶ä¸¦ç™¼æ•¸ç‚º 10
      const limit = pLimit(10);

      const promises = batch.messages.map((message) =>
        limit(async () => {
          try {
            // ä¸¦ç™¼è™•ç†æ¶ˆæ¯
            await processMessage(message);
            resolveOffset(message.offset);
          } catch (error) {
            console.error(`è™•ç†å¤±æ•—: ${message.offset}`, error);
          }
        })
      );

      // ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯è™•ç†å®Œæˆ
      await Promise.all(promises);
      
      // ç™¼é€å¿ƒè·³
      await heartbeat();
    }
  });
}

async function processMessage(message: any) {
  // æ¨¡æ“¬ç•°æ­¥è™•ç†
  await new Promise(resolve => setTimeout(resolve, Math.random() * 1000));
  console.log(`è™•ç†å®Œæˆ: offset ${message.offset}`);
}

run();
```

**æ³¨æ„**ï¼š
- ä¸¦ç™¼è™•ç†æœƒæ‰“äº‚æ¶ˆæ¯é †åº
- é©ç”¨æ–¼é †åºä¸é‡è¦çš„å ´æ™¯
- è¦æ§åˆ¶ä¸¦ç™¼æ•¸ï¼Œé¿å…è³‡æºè€—ç›¡

## éŒ¯èª¤è™•ç†èˆ‡é‡è©¦

### æ–¹å¼ 1ï¼šç«‹å³é‡è©¦

```typescript
async function processWithRetry(message: any, maxRetries = 3) {
  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      await processMessage(message);
      return;  // æˆåŠŸ
    } catch (error) {
      if (attempt === maxRetries) {
        console.error('é‡è©¦æ¬¡æ•¸ç”¨ç›¡:', error);
        throw error;
      }
      console.log(`é‡è©¦ ${attempt + 1}/${maxRetries}`);
      await new Promise(resolve => setTimeout(resolve, 1000 * attempt));
    }
  }
}
```

### æ–¹å¼ 2ï¼šæ­»ä¿¡éšŠåˆ—

å‰µå»º `examples/consumer-advanced/03-dead-letter-queue.ts`ï¼š

```typescript
import { Kafka } from 'kafkajs';

const kafka = new Kafka({
  clientId: 'dlq-demo',
  brokers: ['localhost:9092']
});

const consumer = kafka.consumer({ 
  groupId: 'dlq-group',
  autoCommit: false 
});

const dlqProducer = kafka.producer();

async function run() {
  await Promise.all([
    consumer.connect(),
    dlqProducer.connect()
  ]);

  await consumer.subscribe({ topic: 'orders', fromBeginning: false });

  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      try {
        // å˜—è©¦è™•ç†
        await processMessage(message);
        
        // æˆåŠŸå¾Œæäº¤
        await consumer.commitOffsets([{
          topic,
          partition,
          offset: (parseInt(message.offset) + 1).toString()
        }]);

      } catch (error) {
        console.error('è™•ç†å¤±æ•—ï¼Œç™¼é€åˆ°æ­»ä¿¡éšŠåˆ—', error);
        
        // ç™¼é€åˆ°æ­»ä¿¡éšŠåˆ—
        await dlqProducer.send({
          topic: 'orders-dlq',  // æ­»ä¿¡éšŠåˆ— Topic
          messages: [{
            key: message.key?.toString(),
            value: message.value?.toString(),
            headers: {
              'original-topic': topic,
              'original-partition': partition.toString(),
              'original-offset': message.offset,
              'error-message': (error as Error).message,
              'failed-at': Date.now().toString()
            }
          }]
        });

        // æäº¤ offsetï¼ˆè·³éæ­¤æ¶ˆæ¯ï¼‰
        await consumer.commitOffsets([{
          topic,
          partition,
          offset: (parseInt(message.offset) + 1).toString()
        }]);
      }
    }
  });
}

async function processMessage(message: any) {
  // æ¨¡æ“¬éš¨æ©Ÿå¤±æ•—
  if (Math.random() < 0.2) {
    throw new Error('è™•ç†å¤±æ•—');
  }
  console.log(`è™•ç†æˆåŠŸ: ${message.offset}`);
}

run();
```

## å¯¦ç¾å¯é çš„ Consumer

å‰µå»º `examples/consumer-advanced/04-reliable-consumer.ts`ï¼š

```typescript
import { Kafka, EachMessagePayload } from 'kafkajs';

const kafka = new Kafka({
  clientId: 'reliable-consumer',
  brokers: ['localhost:9092']
});

class ReliableConsumer {
  private consumer;
  private producer;  // ç”¨æ–¼æ­»ä¿¡éšŠåˆ—
  private running = false;
  private messageHandler: (message: any) => Promise<void>;

  constructor(
    groupId: string,
    topic: string,
    handler: (message: any) => Promise<void>
  ) {
    this.consumer = kafka.consumer({
      groupId,
      autoCommit: false,
      sessionTimeout: 30000,
      heartbeatInterval: 3000
    });

    this.producer = kafka.producer({ idempotent: true });
    this.messageHandler = handler;
  }

  async start() {
    await Promise.all([
      this.consumer.connect(),
      this.producer.connect()
    ]);

    this.running = true;

    await this.consumer.subscribe({ 
      topic: 'orders',
      fromBeginning: false 
    });

    await this.consumer.run({
      eachMessage: async (payload) => {
        await this.handleMessage(payload);
      }
    });
  }

  private async handleMessage({ 
    topic, 
    partition, 
    message 
  }: EachMessagePayload) {
    const maxRetries = 3;
    let lastError: Error | null = null;

    for (let attempt = 0; attempt < maxRetries; attempt++) {
      try {
        // è§£ææ¶ˆæ¯
        const parsedMessage = JSON.parse(
          message.value?.toString() || '{}'
        );

        // èª¿ç”¨ç”¨æˆ¶å®šç¾©çš„è™•ç†å™¨
        await this.messageHandler(parsedMessage);

        // æˆåŠŸï¼šæäº¤ offset
        await this.consumer.commitOffsets([{
          topic,
          partition,
          offset: (parseInt(message.offset) + 1).toString()
        }]);

        console.log(`âœ… æˆåŠŸè™•ç†: offset ${message.offset}`);
        return;

      } catch (error) {
        lastError = error as Error;
        console.error(`âŒ å˜—è©¦ ${attempt + 1}/${maxRetries} å¤±æ•—:`, error);

        if (attempt < maxRetries - 1) {
          // æŒ‡æ•¸é€€é¿
          await new Promise(resolve => 
            setTimeout(resolve, Math.pow(2, attempt) * 1000)
          );
        }
      }
    }

    // æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼šç™¼é€åˆ°æ­»ä¿¡éšŠåˆ—
    console.error('é‡è©¦æ¬¡æ•¸ç”¨ç›¡ï¼Œç™¼é€åˆ°æ­»ä¿¡éšŠåˆ—');
    await this.sendToDeadLetterQueue(topic, partition, message, lastError!);

    // æäº¤ offsetï¼ˆè·³éæ­¤æ¶ˆæ¯ï¼‰
    await this.consumer.commitOffsets([{
      topic,
      partition,
      offset: (parseInt(message.offset) + 1).toString()
    }]);
  }

  private async sendToDeadLetterQueue(
    topic: string,
    partition: number,
    message: any,
    error: Error
  ) {
    await this.producer.send({
      topic: `${topic}-dlq`,
      messages: [{
        key: message.key?.toString(),
        value: message.value?.toString(),
        headers: {
          'original-topic': topic,
          'original-partition': partition.toString(),
          'original-offset': message.offset,
          'error-message': error.message,
          'failed-at': Date.now().toString()
        }
      }]
    });
  }

  async stop() {
    this.running = false;
    await Promise.all([
      this.consumer.disconnect(),
      this.producer.disconnect()
    ]);
    console.log('âœ… Consumer å·²åœæ­¢');
  }
}

// ä½¿ç”¨ç¤ºä¾‹
async function main() {
  const consumer = new ReliableConsumer(
    'reliable-group',
    'orders',
    async (order) => {
      // æ¥­å‹™é‚è¼¯
      console.log(`è™•ç†è¨‚å–®: ${order.orderId}`);
      
      // æ¨¡æ“¬éš¨æ©Ÿå¤±æ•—
      if (Math.random() < 0.1) {
        throw new Error('è™•ç†å¤±æ•—');
      }
    }
  );

  await consumer.start();

  // å„ªé›…é—œé–‰
  process.on('SIGINT', async () => {
    console.log('\næ­£åœ¨é—œé–‰...');
    await consumer.stop();
    process.exit(0);
  });
}

main();
```

## æœ€ä½³å¯¦è¸

### 1. å†ªç­‰è™•ç†

```typescript
// ä½¿ç”¨å”¯ä¸€ ID æª¢æŸ¥æ˜¯å¦å·²è™•ç†
const processedIds = new Set();

async function idempotentProcess(message: any) {
  const messageId = message.key.toString();
  
  if (processedIds.has(messageId)) {
    console.log('æ¶ˆæ¯å·²è™•ç†ï¼Œè·³é');
    return;
  }
  
  await processMessage(message);
  processedIds.add(messageId);
}
```

### 2. ç›£æ§ Lag

```typescript
// å®šæœŸæŸ¥çœ‹ Consumer Lag
setInterval(async () => {
  const admin = kafka.admin();
  await admin.connect();
  
  const lag = await admin.fetchOffsets({ groupId: 'my-group' });
  console.log('Consumer Lag:', lag);
  
  await admin.disconnect();
}, 60000);  // æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
```

## å°çµ

æœ¬ç« å­¸ç¿’äº†ï¼š

1. **æäº¤ç­–ç•¥**ï¼šè‡ªå‹• vs æ‰‹å‹•ï¼Œæ¬Šè¡¡æ€§èƒ½å’Œå¯é æ€§
2. **Offset é‡ç½®**ï¼šearliest vs latest
3. **ä¸¦ç™¼è™•ç†**ï¼šæé«˜ååé‡
4. **éŒ¯èª¤è™•ç†**ï¼šé‡è©¦å’Œæ­»ä¿¡éšŠåˆ—
5. **å¯é  Consumer**ï¼šå®Œæ•´çš„éŒ¯èª¤è™•ç†æ–¹æ¡ˆ

## ä¸‹ä¸€æ­¥

ğŸ‘‰ [ä¸‹ä¸€ç« ï¼š11 - Consumer æœ€ä½³å¯¦è¸](../11-consumer-best-practices/README.md)

---

[è¿”å›ç›®éŒ„](../README.md)

